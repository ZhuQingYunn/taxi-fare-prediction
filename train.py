import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import r2_score, mean_squared_error
import xgboost as xgb
import joblib
import os
import warnings

warnings.filterwarnings('ignore')

# ---------------------- æ ¸å¿ƒé…ç½®ï¼ˆæ•°æ®è·¯å¾„ä¸º train.csvï¼‰----------------------
DATA_PATH = "train.csv"  # æ•°æ®æ–‡ä»¶ï¼štrain.csv
MODEL_DIR = "models"  # æ¨¡å‹ä¿å­˜ç›®å½•
RANDOM_STATE = 42  # éšæœºç§å­
TEST_SIZE = 0.2  # æµ‹è¯•é›†æ¯”ä¾‹

# XGBoost åŸç”Ÿå‚æ•°ï¼ˆæœ€åŸºç¡€é…ç½®ï¼Œå…¼å®¹æ‰€æœ‰ç‰ˆæœ¬ï¼‰
XGB_PARAMS = {
    'eta': 0.08,  # å­¦ä¹ ç‡
    'max_depth': 5,
    'min_child_weight': 1,
    'subsample': 0.8,
    'colsample_bytree': 0.8,
    'objective': 'reg:squarederror',
    'seed': RANDOM_STATE,
    'silent': 1  # é™é»˜æ¨¡å¼
}
NUM_BOOST_ROUNDS = 200  # å›ºå®šè¿­ä»£æ¬¡æ•°ï¼ˆå»æ‰æ—©åœï¼Œé¿å…å…¼å®¹é—®é¢˜ï¼‰


# ---------------------- æ•°æ®åŠ è½½ä¸é¢„å¤„ç† ----------------------
def load_and_preprocess_data(data_path):
    print(f"ğŸ“Š åŠ è½½æ•°æ®ï¼š{data_path}")
    if not os.path.exists(data_path):
        raise FileNotFoundError(f"âŒ æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶ï¼š{data_path}")

    df = pd.read_csv(data_path)

    # å¿…è¦åˆ—æ£€æŸ¥
    required_cols = ['distance_traveled', 'num_of_passengers', 'fare']
    missing_cols = [col for col in required_cols if col not in df.columns]
    if missing_cols:
        raise ValueError(f"âŒ æ•°æ®ç¼ºå°‘åˆ—ï¼š{missing_cols}")

    # æ•°æ®æ¸…æ´—
    df = df.dropna(subset=required_cols)
    df = df[(df['distance_traveled'] >= 0.1) & (df['distance_traveled'] <= 100.0)]
    df = df[(df['num_of_passengers'] >= 1) & (df['num_of_passengers'] <= 6)]
    df = df[(df['fare'] >= 0) & (df['fare'] <= 500.0)]

    # ç‰¹å¾å·¥ç¨‹
    df['distance_sq'] = df['distance_traveled'] ** 2
    df['passenger_distance'] = df['num_of_passengers'] * df['distance_traveled']
    df['is_high_passenger'] = (df['num_of_passengers'] >= 3).astype(int)

    print(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼Œæœ‰æ•ˆæ ·æœ¬æ•°ï¼š{len(df)}")
    return df


# ---------------------- æ¨¡å‹è®­ç»ƒï¼ˆæœ€åŸºç¡€å†™æ³•ï¼Œæ— ä»»ä½•å…¼å®¹é—®é¢˜ï¼‰----------------------
def train_model(df):
    feature_cols = [
        'distance_traveled',
        'num_of_passengers',
        'distance_sq',
        'passenger_distance',
        'is_high_passenger'
    ]
    X = df[feature_cols]
    y = df['fare']

    # åˆ’åˆ†æ•°æ®é›†
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, shuffle=True
    )
    print(f"ğŸ“ˆ è®­ç»ƒé›†ï¼š{len(X_train)} æ ·æœ¬ï¼Œæµ‹è¯•é›†ï¼š{len(X_test)} æ ·æœ¬")
    # åˆ’åˆ†è®­ç»ƒé›†/æµ‹è¯•é›†åï¼Œæ·»åŠ ï¼š
    X_test.to_csv("models/X_test.csv", index=False)
    y_test.to_csv("models/y_test.csv", index=False)
    # æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)

    # è½¬æ¢ä¸º XGBoost åŸç”Ÿ DMatrix æ ¼å¼
    dtrain = xgb.DMatrix(X_train_scaled, label=y_train)
    dtest = xgb.DMatrix(X_test_scaled, label=y_test)

    # è®­ç»ƒæ¨¡å‹ï¼ˆå»æ‰æ—©åœã€å»æ‰æ‰€æœ‰å¤æ‚å‚æ•°ï¼Œä»…ä¿ç•™æ ¸å¿ƒï¼‰
    print("ğŸš€ è®­ç»ƒ XGBoost æ¨¡å‹...")
    model = xgb.train(
        params=XGB_PARAMS,
        dtrain=dtrain,
        num_boost_round=NUM_BOOST_ROUNDS,
        verbose_eval=10  # æ¯10è½®æ‰“å°ä¸€æ¬¡æ—¥å¿—
    )

    # é¢„æµ‹ï¼ˆå»æ‰ ntree_limitï¼Œç›´æ¥é¢„æµ‹ï¼‰
    y_pred = model.predict(dtest)

    # è¯„ä¼°
    r2 = r2_score(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    print(f"\nğŸ“Š æ¨¡å‹æ€§èƒ½ï¼š")
    print(f"RÂ² åˆ†æ•°ï¼š{r2:.4f}")
    print(f"RMSEï¼š{rmse:.2f} ç¾å…ƒ")

    return model, scaler, r2, rmse, feature_cols


# ---------------------- æ¨¡å‹ä¿å­˜ ----------------------
def save_model(model, scaler, r2, rmse, feature_cols):
    if not os.path.exists(MODEL_DIR):
        os.makedirs(MODEL_DIR)

    # ä¿å­˜æ¨¡å‹ï¼ˆåŸç”Ÿæ ¼å¼ï¼‰ã€æ ‡å‡†åŒ–å™¨ã€æŒ‡æ ‡
    model.save_model(os.path.join(MODEL_DIR, "best_model_XGBoost.model"))
    joblib.dump(scaler, os.path.join(MODEL_DIR, "scaler.pkl"))

    with open(os.path.join(MODEL_DIR, "model_metrics.txt"), 'w', encoding='utf-8') as f:
        f.write(f"æ¨¡å‹ï¼šXGBoostï¼ˆå…¼å®¹ä½ç‰ˆæœ¬ï¼‰\n")
        f.write(f"è®­ç»ƒæ—¶é—´ï¼š{pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"RÂ²ï¼š{r2:.4f}\n")
        f.write(f"RMSEï¼š{rmse:.2f} ç¾å…ƒ\n")
        f.write(f"è¿­ä»£è½®æ•°ï¼š{NUM_BOOST_ROUNDS}\n")
        f.write(f"ç‰¹å¾åˆ—ï¼š{feature_cols}")

    print(f"\nğŸ’¾ æ¨¡å‹ä¿å­˜è‡³ï¼š{MODEL_DIR}")


# ---------------------- ä¸»å‡½æ•° ----------------------
if __name__ == "__main__":
    try:
        print("=" * 50)
        print("ğŸš• çº½çº¦å‡ºç§Ÿè½¦è½¦è´¹é¢„æµ‹ - æ¨¡å‹è®­ç»ƒ")
        print("=" * 50)

        df = load_and_preprocess_data(DATA_PATH)
        model, scaler, r2, rmse, feature_cols = train_model(df)
        save_model(model, scaler, r2, rmse, feature_cols)

        print("\nğŸ‰ è®­ç»ƒå®Œæˆï¼")
        print("=" * 50)
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥ï¼š{str(e)}")
        raise